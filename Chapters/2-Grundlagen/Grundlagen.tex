%=========================================
% 	   Grundlagen     		 =
%=========================================
\chapter{Grundlagen}
\label{ch:grundlagen}
Um die Cloud-Native-Architektur zu betrachten, gilt es erst einige Grundlagen zu erarbeiten. In diesem Kapitel wird zunächst der Begriff Cloud abgegrenzt. Dabei wird auch der Unterschied von \textit{Public} und \textit{Private Cloud} betrachtet. Daraus wird anschließend eine Liste von Anforderungen an eine Cloud-Architektur formuliert. Aufbauend auf dieser wird abschließend die Cloud Native Architektur definiert und deren Eigenschaften betrachtet. 

\section{Die Cloud}
Der Begriff Cloud ist heutzutage sowohl in Fachliteratur, als auch in IT-fernen Bereichen vorzufinden. Dabei wird der Begriff \textit{Cloud} als Oberbegriff für neue Technologien und Produkte verwendet.\\ 
Dafür gilt es zunächst zu ermitteln, in welcher Umgebung Cloud-Applikationen betrieben werden.\\\\
Das National Institute of Standards and Technology (NIST) definiert \textit{Cloud Computing}, als ein Model für allgegenwärtigen Zugriff auf eine geteilte Menge von Rechenleistung . Der all-gegenwärtige Zugriff wird durch einen sogenannten \textit{Service-Provider} bereitgestellt. Der Kunde kann demnach flexibel Rechenleistung dazu kaufen, oder die bereitgestellte Rechenleistung verringern. Hierfür ist keine menschliche Kommunikation von Nöten. Deshalb wird dies von der NIST als \textit{On-demand self-service} bezeichnet.\\
Um On-demand self-service zu ermöglichen, nutzt der Service-Provider das sogenannte \textit{Resource-Pooling}. Hierfür teilt der Service-Provider dem Kunden automatisiert Rechenleistung zu. Der Kunde hat hierbei nur begrenzt Einfluss auf den Standort oder die genaue Maschine, welche die Leistung zur Verfügung stellt.\cite{mell_nist_2011}\\
Der Kunde bezahlt folglich für Rechenleistung, und nicht für einzelne Maschinen. Die Cloud kann demnach als Paradigma für das Bereitstellen von Services über das Internet gesehen werden \cite{avram_advantages_2014}.

\subsection{Service Modelle}
Verschiedene Geschäftsmodelle des Cloud-Computings setzen unterschiedliche Grade der Abstraktion um. Hierbei wird zwischen den Service Modellen \textit{Infrastructure as a Service}, \textit{Plattform as a Service} und \textit{Software as a Service} unterschieden. Im Nachfolgenden werden diese Modelle voneinander abgegrenzt.
\subsubsection{Infrastructure as a Service}
Das sogenannte\textit{ Infrastructure as a Service (IaaS)}\cite{mell_nist_2011} stellt Speicher -und Rechenleistung zur Verfügung. Der Kunde muss also nicht selbst physische Hardware betreiben. IaaS kann als Grundschicht unter allen anderen Service Modellen gesehen werden.\\
Mittels Virtualisierung teilt der Service-Provider die physische Hardware auf und teilt sie den Kunden zu. Der  Kunde hat also kein Einfluss auf die zugrundeliegende Infrastruktur. Allerdings kann er Aspekte wie Speicher, Betriebssystem, Middle-Ware und die Anwendung selbst konfigurieren \cite{dimpi_rani_rajiv_kumar_ranjan_comparative_2014}.\\
Auch ein begrenzter Zugriff auf Netzwerkomponenten, wie zum Beispiel die Firewall, kann möglich sein. IaaS stellt demnach die selben Funktionen wie ein traditionelles  Rechen-Center zur Verfügung. Der Kunde muss allerdings kein solches Instandhalten und kann flexibel die Rechenleistung erhöhen oder verringern. \\
Abgerechnet wird in der Regel entsprechend der Nutzungsdauer. In einigen Fällen übernimmt der Service-Provider auch Aufgaben wie die Systemwartung, Datensicherung und das Notfall-Management.\\
Unter anderem sind Microsoft, IBM und Amazon Anbieter von IaaS Angeboten. \cite{simon_lohmann_iaas_nodate} \cite{mell_nist_2011}
\subsubsection{Plattform as a Service}
Das \textit{Plattform as a Service (PaaS)}\cite{mell_nist_2011} Model abstrahiert gegenüber IaaS weitere Aspekte der Cloud. Weiterhin muss der Kunde keine eigene Rechenzentren verwalten und erhält durch den Service Provider flexibel Rechenleistung.\\
Allerdings werden nun auch Aspekte wie das Betriebssystem, die Speicherverwaltung mit Datenbanken und das Netzwerk, durch den Service Provider verwaltet. Der Kunde erhält eine Entwicklungsumgebung in der Cloud, über welche der die Anwendung bereitstellen kann. \\
PaaS ist dafür konzipiert den kompletten Lebenszyklus einer Anwendung zu ermöglichen. Hierzu zählen das bauen, testen, veröffentlichen, verwalten und aktualisieren einer Anwendung \cite{dimpi_rani_rajiv_kumar_ranjan_comparative_2014}. Auf einer höheren Abstraktion kommen neben Entwicklungswerkzeugen, Programmiersprachen, Bibliotheken und Datenbanken auch Container-Techniken dazu.\\
Zu den wichtigsten PaaS Anbietern gehören Amazon, IBM und Microsoft. \cite{simon_lohmann_platform_nodate}\cite{sowmya_layers_2014} \cite{mell_nist_2011} 
\subsubsection{Software as a Service}
\textit{Software as a Service (SaaS)} ist die am meisten abstrahierte Form des Cloud-Computings. Kunden können über das Internet auf Angebote zugreifen, welche von einem Service-Provider zur Verfügung gestellt werden. Die Applikation wird durch den Service-Provider verwaltet, betrieben und aktualisiert. \\
Der Nutzer verwaltet keinen Aspekte der zugrundeliegenden Infrastruktur. Einzig eine konfigurieren der Anwendung selbst kann möglich sein.\\
Typische SaaS-Applikationen im Business Bereich sind \textit{Google G Suite} \cite{noauthor_google_nodate}  und \textit{Microsoft Office 365} \cite{noauthor_office_nodate}. \\
SaaS-Provider rechnen Anwendungen in der Regen anhand bestimmter Parameter, wie die Anzahl der Nutzer, ab. Auch bei SaaS-Angeboten können Kunden bei Bedarf einzelne Dienste oder Funktionen stärker in Anspruch nehmen. \cite{wolfgang_herrmann_saas_nodate} \cite{sowmya_layers_2014} \cite{mell_nist_2011}

\subsection{Deployment Modelle}
Neben dem Grad der Abstraktion stellt auch der Ort der Hardware eine Rolle. Wird diese lokal von dem eigenen Unternehmen verwaltet? Oder ist eine externe Firma dafür verantwortlich? Hierfür existieren einige Modell, welche im Folgenden voneinander differenziert werden:
\begin{description}
    \item [on premise] \hfill \\ Auch als on-premise Cloud oder private Cloud bezeichnet \cite{dimpi_rani_rajiv_kumar_ranjan_comparative_2014}. Die Infrastruktur wird exklusiv von einer Organisation genutzt \cite{mell_nist_2011}. Die Verwaltung der Infrastruktur kann ebenfalls durch diese Organisation erfolgen. Alternativ kann eine externe Firma damit beauftragt werden. Die benötigte Hardware befindet sich entweder in-house (\textit{on-premise}) oder oder wird \textit{off-premise} extern bereitgestellt \cite{zwicker_saas_nodate}.
    \item [public Cloud] \hfill \\ Die Infrastruktur wird durch einen Dritt-Anbieter über das Internet zur Verfügung gestellt. Die gesamte Hardware ist im Besitzt des Service-Providers. Die selbe Hardware, Speicher und Netzwerk-Geräte des Service-Providers werden mit anderen Kunden geteilt. Beispiele hierfür wären Microsoft Azure oder Amazon Web Services. \cite{microsoft_public_nodate} \cite{mell_nist_2011}
    \item [hybrid Cloud] \hfill \\ Hybrid Clouds bestehen aus mehreren getrennten Cloud-Infrastrukturen. Daten und Applikationen einer hybrid Cloud können zwischen private Cloud und public Cloud Infrastrukturen wechseln. \cite{mell_nist_2011}
\end{description}

\subsection{Vorteile und Möglichkeiten der Cloud}
Jedes der oben genannten Deployment Modelle und Service Modelle hat eine Menge von Eigenschaften gemeinsam. Avram et al. nennt folgende Eigenschaften: (i) pay-per-use (kein fortlaufenden Verpflichtungen), (ii) elastic capacity and the illusion of infinite resources; (iii) self-service-interface; und (iv) resources that are abstracted or virtualized \cite{avram_advantages_2014}. Aus diesen folgend direkt einige Vorteile des Cloud-Computings.
\begin{itemize}
  \item Durch das pay-per-use Modell muss der Kunde nur für die Leistung zahlen, die er tatsächlich benötigt. Dies reduziert Hardware-Kosten, wie auch die Kosten für Verwaltung, Software-Updates und den Strom. Ferner wird weniger Personal für die Verwaltung der IT benötigt. Dies ermöglicht auch kleineren Firmen rechen-aufwändige Analysen zu bewältigen. Letztere waren zuvor nur größeren Firmen vorbehalten. Diese umfangreichen Berechnungen benötigen meist viel Leistung, für einen kurzen Zeitraum. Durch das self-service-interface (iii) und die dynamische Kapazität von Leistung (iv) mit pay-per-use (i) ist dies möglich. Avram et al \cite{avram_advantages_2014} sieht hier zusätzlich eine Möglichkeit für Dritte-Welt-Länder zu dem Westen aufzuschließen. Länder welche traditionell nicht die benötigen Ressourcen gehabt hätten, können nun diese durch Cloud-Computing beziehen.  
  \item Durch das Cloud-Computing können neue Business-Modell umgesetzt werden. Die Cloud ermöglicht einen direkten Zugriff auf Rechenleistung, ohne eine Vorherige Kapital-Anlage des Kunden \cite{noauthor_premise_2020}. Es muss nicht zuvor Geld in ein Rechenzentrum oder zusätzliche IT-Experten investiert werden. Die Zeit bis ein neues Produkt gewinnbringend an den Markt gebracht werden kann, ist demnach deutlich gesunken. Viele Internet-Startups konnten dadurch gegründet und zum Erfolg geführt werden \cite{avram_advantages_2014}. 
  \item Cloud-Computing erleichtert es Firmen, ihre Services zu skalieren. Wächst die Zahl der Nutzer, kann Rechenleistung dazu gekauft werden. Die Kosten steigen daher dynamisch mit der Reichweite des Unternehmens. Auch eine Reaktion auf ein kurzfristigen Anstieg oder Abfall der Nutzer-Anfragen ist möglich. Eines der Ziele des Cloud-Computings besteht darin, Ressourcen mithilfe von Software-APIs je nach Client-Auslastung bei minimaler Interaktion mit dem Service-Provider dynamisch zu vergrößern oder zu verkleinern \cite{avram_advantages_2014}. Dies ist möglich, da die Rechen-Ressourcen durch Software verwaltet werden. Für eine dynamische Skalierung, wird zusätzlich eine passende Software-Architektur benötigt. Hierauf wird in einem späteren Kapitel eingegangen.
  \item Hier ist außerdem der Gedanke aufzugreifen, dass eine Fokussierung auf das Geschäftsmodell möglich wird. Firmen können sich auf ihr eigentliches Geschäftsmodell konzentrieren. Zeit und Geld können in das Geschäftsmodell und nicht in die IT-Infrastruktur investiert werden.
\end{itemize}

\section{Anforderungen}
Nachdem wir im letzten Abschnitt den Begriff Cloud-Computing erläutert haben, gilt es nun Cloud-Applikationen zu betrachten. Die meisten der zuvor genannten Vorteile des Cloud-Computings können nur durch eine passende Software-Architektur vollkommen ausgeschöpft werden. Beim Entwurf der Architektur einer Cloud-Applikation müssen einige Aspekte beachtet werden. Folgende Anforderungen haben sich dabei herauskristallisiert:
\begin{itemize}
    \item Cloud-Applikationen operieren meist global. Dies heißt nicht nur, dass der Dienst stets über das Internet erreichbar ist. Bei der Betrachtung von globalem Zugriff muss berücksichtigt werden, dass der Dienst in lokalen Rechenzentren dupliziert werden muss \cite{gannon_cloud-native_2017}. Nur so kann eine geringe Latenz garantiert werden. Dabei führt die Integrität von Daten häufig zu Problem. Die persönlichen Daten müssen dem Nutzer stets zur Verfügung stehen. Dem Nutzer darf nicht ersichtlich sein, dass seine Daten an mehreren Orten abgespeichert sind.  
    \item Cloud-Applikationen müssen mit vielen tausend von parallelen Nutzern operieren können. Neben einer vertikalen Skalierung (mehr und bessere Hardware) wird demnach auch eine horizontale Skalierung nötig \cite{gannon_cloud-native_2017}. Diese wird meist durch Parallelisierung umgesetzt. Eine horizontale Skalierung führt erneut zu einem Fokus auf die Konsistenz und Synchronisation des Systems. 
    \item Die Architektur einer Cloud-Applikationen muss mit der Annahme entwickelt werden, dass die Hardware nicht konstant ist und Fehler vorkommen werden \cite{gannon_cloud-native_2017}. Gannon et Al. \cite{gannon_cloud-native_2017} sieht Probleme bei Applikationen, welche nicht für die Cloud entwickelt wurden. Diese nehmen an, dass die Hardware und das Betriebssystem konstant sind. Der kleinste Fehler im Rechenzentrum oder des Netzwerks führt hier bereits zu Fehlern. 
    \item Cloud-Applikationen müssen die Anforderungen an Verteilte-Systeme beachten \cite{scholl_cloud_2019}. Als Beispiel soll das sogenannte CAP-Theorem erwähnt werden \cite{julianbrowne_brewers_nodate}. Ohne eine Optimierung für den Ausfall einzelner (Teil-)Systeme ist die Anforderung \textit{Partition Tolerance} des CAP-Theorem nicht erfüllt. Es stellt sich also heraus, dass verteilte Systeme eine Reihe von Anforderungen mit sich bringen. Die Architektur muss demnach damit umgehen, dass sich Dienste nicht auf der selben Maschine befinden. Es wird folglich mit einem Netzwerk von Maschinen gearbeitet. Es wird die Welt der Verteilten Systeme betreten. Diese bringt eine Reihe von eigenen Anforderungen mit sich. Oft werden hier Falsch-Annahmen gemacht. Scholl et Al. \cite{scholl_cloud_2019} nennt diese Annahmen  \textit{Fallacies of Distributed Systems}. Dazu gehören Aussagen wie unter Anderem ``the Network is reliable'', ``Latency is zero'' oder ``There is infinite bandwith''. All diese Aspekte führen zu eigenen Anforderungen, welche die Architektur beachten muss.
    \item Die meisten Cloud-Applikationen werden für den Dauerbetrieb entwickelt. Ein zugriff der Nutzer soll stets möglich sein. Hier ist außerdem der Gedanke aufzugreifen, dass Firmen  Verträge mit dem Kunden haben können. Diese beschränken eine mögliche Ausfallzeit auf wenige Stunden im Jahr. Ein Einspielen von Updates darf demzufolge nicht zu einer Downtime für den Nutzer führen. Zudem müssen diese Updates stets getestet werden. Auch Fehler im System oder Abstürze einzelner Knoten dürfen nicht das gesamte System, oder einzelne Funktionen dessen, lahmlegen. Ein umfangreiche Überwachung des Systems und das vorhalten von Ersatzkonten sind nur einige der wichtigen Operationen.\cite{gannon_cloud-native_2017}
\end{itemize}
Zusammenfassend lässt sich sagen, dass Entwickler und Software-Architekten drei großen Herausforderungen der Cloud gegenüber stehen. Zunächst müssen sie verteilte Systeme verstehen. Eine Anforderungen, welche so vorher schon relevant war, allerdings nun noch prominenter in den Vordergrund dritt. Ferner müssen sich Entwickler mit neuen Technologien wie Containern vertraut machen. Und abschließend müssen sie sich bewusst werden, welche Entwurfsmuster in der Cloud funktionieren.

\section{Prinzipien einer Cloud Native Architektur}
Nachdem nun die Begriffe Cloud-Computing und Cloud-Applikationen beleuchtet wurde, kann eine Architektur für die Cloud betrachtet werden. Das Ziel dieser ist, die Möglichkeiten der Cloud maximal zu Nutzen. Traditionelle Infrastrukturen (on-premise Service Modelle) fokussieren sich auf eine feste Anzahl an Hardware-Komponenten und deren Leistung. Der Cloud-Service-Provider teilt die Leistung allerdings flexibel zu. Folglich nutzen Cloud-Native Architekturen Prinzipien, wie das horizontale Skalieren und automatische Ersetzen von abgestürzten Komponenten um Leistung und Elastizität bereit zu stellen.\\
Sowohl Scholl et Al.\cite{scholl_cloud_2019} , als auch Microsoft, Amazon und Google \cite{tom_grey_5_nodate} definieren eine Vielzahl von Eigenschaften einer Cloud-Native Architektur. Im Folgenden werden diese Eigenschaften beleuchtet. 
\subsection{Automatisierung}
Eines der Ziele einer Cloud-Native Architektur ist es, so viele Aspekte des Systems wie möglich zu automatisieren. Scholl et Al.\cite{scholl_cloud_2019} bezeichnet diese Eigenschaft als ''Operational Excellence''. Ganz natürlich kommt hier das Continuous Integration / Continuous Delivery (CI/CD) ins Spiel. CI/CD beschreibt das automatisierte bauen, testen und deployen des Systems. Auch das Zurücksetzen von Updates (\textit{Rollback}) sollte automatisiert werden.\\
Neben diesen Apsekten ist auch das Logging zentral. \\
Eine Überwachung der Anwendung sollte ebenfalls Implementiert werden. Diese führt dazu, dass Fehler frühzeitig erkannt werden können. Darüber hinaus hat diese auch weitere Vorteile.\\
Aspekte der Cloud wie das on-demmand-self-service ermöglichen es, im Code der Anwendung selbst, die Umgebung der Anwendung anzupassen. So kann ein System sich selbst neue Ressourcen buchen und anwenden. Ein Beispiel wäre hier die Speicherplatzüberwachung. Ist eine Speichereinheit ausgelastet, müsste ein on-premise Deployment Modell auf physische Hardware-Erweiterung warten. Durch das Cloud-Computing kann nun die Software selbst neue Ressourcen buchen. Dies ist führt zu weniger Down-Time und reduziert die Aufgabe von Personal. Grund-Voraussetzung dafür ist eine planvolle Automatisierung und Überwachung.\\ 
Hinzu kommt außerdem, dass eine Cloud-Native-Applikation ein verteiltes System ist. Dadurch ergeben sich neue Fehler-Quellen. Diese sind oft schwer zu testen. Eine Cloud-Native Architektur sollte auch auf diese automatisiert reagieren.\\
Weiterhin lässt sich das Skalieren des Systems automatisieren. Durch das herauf Skalieren bleibt das System weiter erreichbar, und durch das herunter Skalieren werden die Kosten reduziert.\\
Scholl et Al.\cite{scholl_cloud_2019} nennt zudem die Anforderung "Document everything". Eine Automatisierung und Überwachung können nur dann sinnvoll und funktional sein, wenn diese auch klar dokumentiert sind. Jedes Team-Mitglied muss bewusst sein, wie die Umgebung der Anwendung definiert ist und wie die Anwendung auf Ereignisse automatisiert reagiert. Ebenso wie die Anwendung selbst, lässt sich auch das Dokumentieren, mit geeigneten Tools, automatisieren.
\subsection{Sicherheit}
Typisch für Cloud Native Architekturen ist der \textit{defense-in-depth} Sicherheits-Ansatz. Hierbei ist nicht eine zentrale Instanz für die Sicherheit des Systems zuständig. Jede Kommunikation zwischen einzelnen Komponenten des Systems ist abgesichert. Dies schließt eine sichere Verbindung zu dem Source-Code-Repository mit ein. Jede einzelne Komponente, auch wenn diese intern ist, sollte kein Grund-Vertrauen zu anderen Komponenten des Systems haben. Jede Komponente sollte sich vor den anderen Schützen.
\subsection{Sinnvoller Einsatz von State}
Die Speicherung von \textit{State} beschreibt zum einen die Speicherung von Nutzerdaten (z.B. die Einträge in dem Warenkorb des Nutzers) und den Status des Systems (z.B. welche Version des Source-Codes wird gerade im Produktiv-System genutzt). Google beschreibt den State als den schwersten Aspekt von verteilten Systemen \cite{tom_grey_5_nodate}. Daher gilt es nur jene Komponenten des Systems mit State zu füllen, welche diesen tatsächlich benötigen. Komponenten ohne State können erstens einfacher skaliert werden. Zweitens ist bei fehlgeschlagenen Instanzen kein State verloren gegangen. Abschließend ist ein Roll-Back zu einer früheren Version mit \textit{stateless} Komponenten problemloser. Auch hier muss kein inkompatibler State vermieden werden. 



\section{Cloud-Native Technologien}
Die Umsetzung einer Cloud-Native Architektur kann mit \textit{Cloud-Native Technologien} gelöst werden.\\ Die \textit{Cloud Native Computing Foundation} (CNCF) wurde 2015 durch die Linux Foundation gegründet \cite{cncf_new_2015}. Weitere Gründungsmitglieder waren unter anderem Google, RedHat, Huawei und Twitter. Neben Beispiel-Projekten, einer Liste von Cloud-Native Technologien und weiteren Ressourcen definiert die CNCF auch Cloud-Native-Technologien. Diese ''[...] ermöglichen es Unternehmen, skalierbare Anwendungen in modernen, dynamischen Umgebungen zu implementieren und zu betreiben.''\cite{cloud_native_computing_foundation_cncftoc_nodate} Als moderne Umgebungen sind Deployment-Modelle wie die private Cloud, public Cloud und hybrid Cloud zu verstehen. Als Best-Practices sieht die CNCF unter anderem \textit{Micro-Services}, \textit{Container} und \textit{Services-Meshes}, auf welche im Folgenden eingegangen werden soll. Das nachfolgende Kapitel orientiert sich inhaltlich am Buch ''Cloud Native'' von \textit{Scholl, Swanson & Jausovec}.

\subsection{Micro-Services-Architektur}

Eine fundamentale Eigenschaft der Cloud-Native-Architektur ist die \textit{Verteiltheit} ihrer Bestandteile. Ein sogenanntes "verteiltes System" beschreibt eine Zusammenkunft aus mehreren Untersystemen oder Rechnereinheiten, welche nach außen hin wie eine abgeschlossene Einheit funktioniert. Seine Bestandteile sowie deren Aufbau und Funktion sind dem Nutzer des Systems im Idealfall also nicht bewusst. Die Cloud-Native-Architektur verwendet zur Umsetzung der gesamten Funktionalität des zu schaffenden Systems -- und damit seiner Untersysteme -- sogenannte ''Micro-Services''.

Ein Micro-Service ist ein isolierter Kleinstbaustein, welcher im Idealfall genau eine Funktionalität realisiert. Seine Entwicklung, Wartung und Dokumentation werden dabei typischerweise von genau einem Team übernommen. Da jeder einzelne Micro-Service einfacher zu verstehen ist, als das gesamte System, ermöglicht die Micro-Service Architektur also eine Expernbildung in den einzelnen Teams und verhindert somit eine redundante Wissensüberschneidung in den Beauftragten des Systems. Die Teams können ihren Fokus also vollständig auf den ihnen zugeteilten Micro-Service legen.

Zur Kommunikation zwischen einzelnen Micro-Services stellt ein solcher eine klar definierte Schnittstelle bereit, über die Daten ausgetauscht werden können.
\cite{chris_richardson_microservices_nodate}
\begin{figure}[H] 
  \centering
  \includegraphics[width=0.7\textwidth]{Chapters/2-Grundlagen/Graphics/Microservice_Architecture.png}
  \caption{Beispielhafter Aufbau einer Micro-Service Architektur \cite{chris_richardson_microservices_nodate}}
  \label{fig:microservices}
\end{figure}
Entwickler verschiedener Micro-Services benutzen die Schnittstellen der jeweils Anderen, um Datenflüsse abzubilden, die zur Umsetzung des eigenen Micro-Service vonnöten sind. Gleichzeitig stellen sie für andere Micro-Services dem von ihnen entwickelten Micro-Service ebenfalls eine Schnittstelle bereit. 

In der Theorie ist für einen Service \textit{A} ist nicht von Bedeutung, wie ein anderer Service \textit{B} seine Funktionalität realisiert. Seine Schnittstelle und deren Definition ist \textit{A} bekannt. \textit{A} benutzt die Schnittstelle von \textit{B} beispielsweise, um Daten abzufragen, die zur Berechnung einer hypothetischen Funktion $F_A$ benötigt werden. Micro-Service \textit{A} stellt $F_A$ wiederum über eine Schnittstelle nach außen hin zur Verfügung. Die Nutzung einer Schnittstellenfunktion eines Micro-Service erfolgt entweder programmatisch durch den Zugriff eines anderen Micro-Service, oder über Anknüpfung an eine grafische Benutzeroberfläche.
\\

\paragraph{API-Versionen}\mbox{}\\

Diese Kapselung ermöglicht eine einwandfreie, getrennte Weiterentwicklung spezifischer Micro-Services, sofern sich die Schnittstellendefinition verschiedener Versionen nicht unterscheidet. Eine vollständige Äquivalenz zwischen den Schnittstellen verschiedener Micro-Service-Versionen kann jedoch nicht immer gewährleistet werden. 

Zur Lösung dieser Problematik schlägt \textit{Jean-Jacques Dubray} [TODO:Quelle] verschiedene Lösungsansätze vor. Eine Möglichkeit zum Design von Schnittstellen und deren Versionen ist das sogenannte ''Knoten-Modell''. In ihm sind benutzende Micro-Services $B_1, ..., B_n$ einer Schnittstelle \textit{S} fest an ihre Version gebunden. Sie besitzen keinerlei Garantie über die Äquivalenz der Schnittstelle in kommenden Versionen. Ändert sich die Schnittstelle, müssen alle von ihr abhängigen Benutzer ebenfalls manuell überprüft und geändert werden. 
\\
Eine weitere Strategie ist die sogenannte \textit{Punkt-zu-Punkt}-Methode. Ähnlich zum Knoten-Modell sind alle Benutzer einer bestimmten Schnittstelle zunächst an eine bestimmte Version gebunden. Im Gegensatz zum Knoten-Modell bleiben Micro-Services mit veralteten Schnittstellenversionen bei deren Änderung jedoch bestehen. Die Teilnehmer eines vorhandenen Netzes von miteinander verbundenen Micro-Services müssen also nicht sofort auf eine neue Version eingestellt werden. Einzelne Netzteilnehmer können bei Bedarf auf die neue Schnittstellenversion ihrer Partner aktualisiert werden. Die Micro-Services unterschiedlicher Schnittstellenversionen werden dabei parallel betrieben, damit der Zugriff auf veraltete Versionen dauerhaft gewährleistet werden kann.
\\
Die letzte von Dubray vorgeschlagene Lösungstrategie beschreibt das Modell einer ''kompatiblen Versionierung''. Anstelle einer festen Verbindung unterschiedlicher Benutzer an die Schnittstelle einer bestimmten Version sind letztere hier an alle Endpunkte der Schnittstellendefinition gebunden, die zu einem bestimmten Versionszeitpunkt verfügbar waren. Alle Folgeversionen müssen die Idempotenz der Funktionalität dieser Endpunkte hinsichtlich der vorangegangenen Versionen gewährleisten. Um einer Schnittstelle neue Funktionalität hinzuzufügen stellt man deswegen gänzlich neue Endpunkte zur Verfügung. Man spricht konkret von der ''Abwärtskompatibilität'' neuerer Schnittstellenversionen.

\paragraph{Kommunikation unter Micro-Services}\label{grundlagen:microcomms}
\mbox{}\\

Zum Datenaustausch bedienen sich Micro-Services verschiedener Netzwerkkommunikationsmodelle. Dabei unterscheidet man zwischen \textit{interner} oder \textit{horizontaler} und \textit{externer} beziehungsweise \textit{vertikaler} Kommunikation. Während erstere die Kommunikation zwischen Micro-Services einer Gruppierung von Micro-Services beschreibt, stellt letztere den Informationsaustausch zwischen einer solchen Gruppierung und externen Dienstleistern oder Nutzern dar. Eine Möglichzeit zur Umsetzung dieses Modells findet sich in Kapitel [TODO Kubernetes]. \\
Zum konkreten Datenaustausch bedienen sich Micro-Services dem ''Client-Servier'' Modell. Den Kommunikationspartnern steht dabei unter anderem das ''Request-Response''-Modell zur Verfügung [TODO: Grafik]. Ein Microservice \textit{A}, der Daten oder Funktionen bereitstellt, setzt diesen Vorgang über einen Server um. Dieser Server besitzt eine feste Adresse. Er stellt spezifische Datensätze oder Funktionen über einen oder mehrere, festgelegte Endpunkte bereit. Ein Client ist in der Lage, durch die Kombination einer Serveradresse und eines Endpunkt Anfragen nach diesen Daten oder der Ausführung einer Funktion zu stellen(\textit{Request}). Aufgabe des Servers ist es danach, eine sinnvolle Antwort an den Client zurückzusenden (\textit{Response}). Um den Abrufungsprozess weiterhin zu steuern, besitzt ein Client die Möglichkeit, seinen Anfragen bestimmte Parameter hinzuzufügen.
\textit{A}s Schnittstelle $S_A$ setzt sich im ''Response-Request-Modell'' aus allen Endpunkten zusammen, die \textit{A} über seinen Server nach außen hin verfügbar macht. $S_A$ legt weiterhin fest, ob und welche Parameter sowie deren Aufbau ein in ihr enthaltener Endpunkt besitzt. \\
Eine weitere Möglichkeit zum Datenaustausch bietet das ''Publisher-Subscriber-Modell''. In diesem benutzt der Micro-Service \textit{P}, der Daten bereitstellt, einen sogenannten ''Message-Broker''. Dieser Message-Broker stellt eine Art Kanal (\textit{Topic)} zur Verfügung, in welchen \textit{P} Daten oder Nachrichten zur Verfügung stellt. Diesen Vorgang nennt man ''Publishing''. Eine beliebige Anzahl an weiteren Micro-Services $S_0, ..., S_n$ ist in der Lage, sich beim Message-Broker für diese Topic zu registrieren. Wenn \textit{P} eine Nachricht in diesen Kanal hinterlegt, werden alle registrierten Services $S_0,...,S_n$ benachrichtigt. Diese sind dann in der Lage, die so gewonnenen Daten weiterzuverarbeiten.  

- Datenformate

\subsection{Container}

Ein \textit{Container} ist eine Einheit, welche Software, ihre Abhängigkeiten sowie Konfigurationen in sich bündelt. Er wird wiederum innerhalb eines \textit{Container-Abbildes} gebündelt. Ein Container-Abbild ist eine Art Vorlage, die den Erstellungsprozess eines Containers beschreibt. Es enthält weiterhin zusätzliche Informationen, die zur Ausführung der im Container enthaltenen Software benötigt werden. Unter diesen befinden sich unter anderem verschiedene Systemtools und -bibleotheken, sowie eine Laufzeitumgebung. Während Micro-Services die Funktionen einer Cloud-Native-Architektur einzeln realisieren, kapseln Container einzelne Micro-Services wiederum vom Betriebssystem ihres ausführenden Computers ab. Da Container-Abbilder alle nötigen Informationen zur Ausführung von Software beinhalten, können in ihnen gekapselte Micro-Services betriebssystemunabhängig ausgeführt werden, solange auf dem ausführenden System eine sogenannte \textit{Engine} existiert, welche eine Brücke zwischen seinem Betriebssystem und dem Container liefert. Container sind durch diesen Vorgang vom ausführenden System und anderen Containern isoliert. Möchte Software innerhalb eines Container in ihm beinhaltete Daten oder Abhängigkeiten verändern, geschieht das durch die sogenannte ''Copy-on-Write''-Strategy (COW). Die zu verändernden Daten werden zunächst kopiert, bevor sie verändert werden. Je nach Implementierung liefern verschiedene Container-Architekturen unterschiedliche Strategien, um veränderte Daten nachhaltig zu persistieren. In der Micro-Service-Architektur werden diese Stragien in der Regel selten benötigt, da ihre Bestandteile meist zustandslos sind. Durch die in diesem Abschnitt vorgestellten Methoden sind Container in der Lage, ihre Funktion unabhängig der zugrunde liegenden Infrastruktur auszuführen. \cite{docker_what_nodate}. 

\subsection{Service-Meshes}
Um die Vorteile und Eigenschaften eines Service-Meshes zu betrachten, muss zunächst der Begriff Container-Orchestration beleuchtet werden.
\subsubsection{Container-Orchestration}
Werden die Services einer Micro-Service-Architektur in Containern betrieben, können sie unabhängig von einer festen Infrastruktur betrieben werden.\\
Für eine Cloud-Native Architektur ist dies von Vorteil. Denn Container lassen sich mit minimaler Verzögerung starten und stoppen. Zudem können wir horizontal skalieren, indem wir weitere Container starten. Allerdings ergeben sich dadurch einige Probleme. Zunächst ein Mal müssten Container automatisch gestartet, gestoppt und neu gestartet werden. Die Skalierung des Systems, oder im Fehlerfall das Neu starten von Containern darf keine menschliches Aufgabe sein. Denn dies wäre zwangsläufig mit einer zu großen Verzögerung verbunden.\\
Um diese Verzögerung weiter zu reduzieren, könnten bereits Ersatz-Container vorgehalten werden. Im Fehlerfall oder für die Skalierung würde ein Umleiten des Traffics genügen. Dies erfordert ein automatisiertes Load-Balancing.\\
Ein funktionierendes Load-Balancing ist auch für die Container zu Container Kommunikation notwendig. Wird ein Service durch mehrere Container umgesetzt, gilt es zu definieren, welcher dieser Container adressiert werden soll. Dieses Problem wird als internal Load-Balancing bezeichnet. \\\\

Die\textit{ Container Orchestration}\cite{scholl_cloud_2019} behebt diese Probleme. Der \textit{Container-Orchestrator} liegt als eine Schicht über den Containern. Er ist verantwortlich für das starten und stoppen von Containern. Außerdem teilt er flexibel Rechenleistung zu und skaliert das System passend zum bestehenden Traffic.\\
Stehen mehrere Server zur Verfügung, platziert er neue Container auf den Servern mit der niedrigsten Auslastung. Außerdem verschiebt er Container, sollte die Auslastung zunehmen. Auch andere Metriken sind für die Server Auswahl und die Verschiebung der Container möglich.\\
Der Container-Orchestrator überwacht zudem die Gesundheit der Container. Falls nötig startet er diese neu.\\
Abschließend ist er für das interne und externe Load-Balancing verantwortlich. Realisieren mehrere Container den selben Service, leitet der Load-Balancer den Traffic zu dem geeignetsten Container weiter. \\
Neben den genannten Funktionen kann der Container-Orchestrator auch für das State-Management und viele weitere Aspekte des Systems zuständig sein.\\
Scholl et Al.\cite{scholl_cloud_2019} sieht in Google Kubernetes \cite{kubernetes_production-grade_nodate} die populärste Container Orchestrator Wahl.
\subsubsection{Herausforderungen der Micro-Service-Architektur}
Durch die Aufteilung unserer Anwendung in Micro-Services haben wir die Komplexität der einzelnen Bestandteile unserer Architektur reduziert. Man sollte allerdings bedenken, dass gleichzeitig die Komplexität auf dem System-Level zugenommen hat. Jeder Service einer Micro-Service Architektur wird, neben der Business Logik, unter anderem folgendes enthalten \cite{codingwithnana_istio_nodate}:
\begin{itemize}
    \item Jede Service wird eine Retry-Logik umsetzen. Es ist möglich, dass Services nicht erreichbar sind. Ist dies der Fall, muss definiert werden, wie oft eine Verbindungsversuch unternommen wird. Ferner ist eine Ausnahmebehandlung zu ergänzen.
    \item Darüber hinaus ist die Umsetzung der Sicherheit zu beachten. Eine Firewall kann die Anwendung von außen Schützen. Allerdings gilt es das Konzept der in-depth Security umzusetzen. Diese verlangt, dass auch die Kommunikation unter den Containern abgesichert ist. Ferner sollte nicht jeder Container frei mit jedem anderen Container kommunizieren dürfen.
    \item Abschließend ist für die Automatisierung einer Überwachung unumgänglich. Es gilt Metriken zu erfassen und auf Basis dessen zu Handeln. Logger und Logik zum Fehlerfinden müssen ebenfalls in jedem Service integriert werden.
\end{itemize}
Ein Vorteil der Microservice-Architektur war es, dass jedes Team einen klar getrennten Anteil der Anwendung entwickelt. Allerdings muss jedes Team, neben der eigentlichen Business Logik, die zuvor genannten Funktionen entwickeln, verwalten und konfigurieren. Diese Konfiguration kann nicht individuell für jeden Service statt finden, ohne zu Fehlern und Problem zu führen. Außerdem wird die Fehlersuche deutlich erschwert. Eine Isolation von Fehlern in einem einzigen Service ist nicht mehr sichergestellt.

\subsubsection{Vorgehensweise von Service-Meshes}
\textit{Service Meshes} nutzen einen sogenannten \textit{Sidecar-Proxy} \cite{codingwithnana_istio_nodate}. Dieser wird in Abbildung \ref{fig:service-mesh} dargestellt. Jedem Container wird ein weiterer Container hinzugefügt. Dieser fungiert als Proxy. Er nimmt demnach Verbindungen an und leitet sie zu dem eigentlichen Service weiter. Diese Side-Car-Proxys werden durch die \textit{Control-Plane} verwaltet und in die einzelnen Services injiziert. Ein neuer Service muss demnach die zuvor genannten Probleme, wie Sicherheit oder die Retry-Logic nicht erneut umsetzen. Außerdem ist die Control-Plane eine zentrale Schnittstelle um die gesamte Anwendung zu konfigurieren und zu Überwachen. Neben den genannten Funktionen, kann ein Service-Mesh auch weitere Aspekte, wie zum Beispiel das Load-Balancing, lösen. Meist wird an dieser stelle aber auf einem Container-Orchestrator wie Kubernetes \cite{kubernetes_production-grade_nodate} aufgebaut. Eine mögliches Service-Mesh wäre zum Beispiel Istio \cite{istio_istio_nodate}. 
\begin{figure}[bth] 
  \centering
  \includegraphics[width=0.7\textwidth]{Chapters/2-Grundlagen/Graphics/service-meshes.png}
  \caption{Service-Meshes am Beispiel Istio \cite{codingwithnana_istio_nodate}}
  \label{fig:service-mesh}
\end{figure}

\subsection{Organisation}

Organisation, CI/CD und DevOps

- In der Praixs ein Repo pro Service
- Konfigurationen über Environment
- Dependencies über Package Manager
- 
